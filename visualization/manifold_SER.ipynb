{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning for Speech Emotion Recognition\n",
    "## Efthymios Tzinis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the appropriate modules \n",
    "import os, sys, glob\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "sys.path.append('../')\n",
    "import config\n",
    "sys.path.append(config.BASE_PATH)\n",
    "from dataloader import fused_features_IEMOCAP as IEMOCAP_loader\n",
    "\n",
    "sys.path.append(config.PATTERN_SEARCH_MDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Folds Generator\n",
    "def get_dataset_in_one_array(features_dic,\n",
    "                             included_sessions=['Ses01', 'Ses02']):\n",
    "    speaker_indices = {}\n",
    "    x_all_list = []\n",
    "    Y_all = []\n",
    "    prev_ind = 0\n",
    "    for te_speaker, te_data in features_dic.items():  \n",
    "        ses_name = te_speaker[:-1]\n",
    "        if not ses_name in included_sessions:\n",
    "            continue\n",
    "        x_all_list.append(te_data['x'])\n",
    "        Y_all += te_data['y']\n",
    "        this_speaker_samples = len(te_data['y'])\n",
    "        \n",
    "        speaker_indices[te_speaker] = (prev_ind, prev_ind + this_speaker_samples)\n",
    "        prev_ind += this_speaker_samples\n",
    "        X_all = np.concatenate(x_all_list, axis=0)\n",
    "    return X_all, Y_all, speaker_indices, len(included_sessions)\n",
    "\n",
    "\n",
    "def generate_session_folds(X_all, Y_all, features_dic, speaker_indices):\n",
    "    sorted_speakers = sorted(speaker_indices)\n",
    "    for i in np.arange(0, len(sorted_speakers), 2):\n",
    "        sp1 = sorted_speakers[i]\n",
    "        sp2 = sorted_speakers[i+1]\n",
    "        \n",
    "        session_name = sp1[:-1]\n",
    "        \n",
    "        st1, et1 = speaker_indices[sp1]\n",
    "        st2, et2 = speaker_indices[sp2]\n",
    "        \n",
    "        Y_te = features_dic[sp1]['y'] + features_dic[sp2]['y']\n",
    "        X_te = np.concatenate([X_all[st1:et1, :], X_all[st2:et2, :]], axis=0)\n",
    "        \n",
    "        x_tr_list = []\n",
    "        Y_tr = []\n",
    "        for sp in sorted_speakers:\n",
    "            if sp == sp1 or sp == sp2:\n",
    "                continue\n",
    "            st, et = speaker_indices[sp] \n",
    "            x_tr_list.append(X_all[st:et, :])\n",
    "            Y_tr += features_dic[sp]['y']\n",
    "            \n",
    "        X_tr = np.concatenate(x_tr_list, axis=0)    \n",
    "        \n",
    "        yield session_name, X_te, Y_te, X_tr, Y_tr \n",
    "\n",
    "def generate_folds(features_dic,\n",
    "                   group_by = 'speaker'):\n",
    "    if group_by == 'speaker':\n",
    "        for te_speaker, te_data in features_dic.items():\n",
    "            x_tr_list = []\n",
    "            Y_tr = []\n",
    "            for tr_speaker, tr_data in features_dic.items():\n",
    "                if tr_speaker == te_speaker:\n",
    "                    continue\n",
    "                x_tr_list.append(tr_data['x'])\n",
    "                Y_tr += tr_data['y']\n",
    "\n",
    "            X_tr = np.concatenate(x_tr_list, axis=0)\n",
    "            yield te_speaker, te_data['x'], te_data['y'], X_tr, Y_tr\n",
    "     \n",
    "    elif group_by == 'session':\n",
    "        already_tested = []\n",
    "        for te_speaker, te_data in features_dic.items():\n",
    "            if not (te_speaker[:-1] in already_tested) :\n",
    "                already_tested.append(te_speaker[:-1])\n",
    "            else:\n",
    "                continue\n",
    "            X_val =  te_data['x']\n",
    "            Y_val = te_data['y']\n",
    "            x_tr_list = []\n",
    "            Y_tr = []\n",
    "            ses_name = te_speaker[:-1]\n",
    "            for tr_speaker, tr_data in features_dic.items():\n",
    "                if tr_speaker == te_speaker:\n",
    "                    continue\n",
    "                if tr_speaker[:-1] == ses_name:\n",
    "                    val_speaker = tr_speaker\n",
    "                    X_val = tr_data['x']\n",
    "                    Y_val = tr_data['y']\n",
    "                    continue\n",
    "                x_tr_list.append(tr_data['x'])\n",
    "                Y_tr += tr_data['y']\n",
    "\n",
    "            X_tr = np.concatenate(x_tr_list, axis=0)\n",
    "            X_ses = np.concatenate([te_data['x'], X_val], axis=0)\n",
    "            Y_ses = te_data['y'] + Y_val\n",
    "            yield ses_name, X_ses, Y_ses, X_tr, Y_tr\n",
    "            \n",
    "def fuse_excited_happiness(l):\n",
    "    return ['happy + excited' \n",
    "            if (e == 'excited' or e == 'happy') \n",
    "            else e for e in l ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class IEMOCAPData(Dataset):\n",
    "    def __init__(self, X,):\n",
    "        self.X_high = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X_high.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_high[idx]\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, hidden_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, in_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        x = self.decoder(h)\n",
    "        return x, h\n",
    "\n",
    "\n",
    "class AE(object):\n",
    "    def __init__(self, original_dim, target_dim, batch_size=32, learning_rate=1e-4, num_epochs=100, early_stop=10):\n",
    "        self.original_dim = original_dim\n",
    "        self.target_dim = target_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stop = early_stop\n",
    "        self.dvc = 'cuda'\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        iemo_data = IEMOCAPData(X)\n",
    "\n",
    "        dataloader = DataLoader(iemo_data, batch_size=self.batch_size, shuffle=True)\n",
    "        model = autoencoder(self.original_dim, self.target_dim).to(self.dvc)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        patience = 0\n",
    "        prev_avg_loss = np.Inf\n",
    "        for epoch in range(self.num_epochs):\n",
    "            avg_loss, i = 0, 0\n",
    "            for data in dataloader:\n",
    "                data = torch.Tensor(data).type(torch.FloatTensor).to(self.dvc)\n",
    "                # ===================forward=====================\n",
    "                output, hidden = model(data)\n",
    "        #         loss = criterion(output, data, hidden)\n",
    "                loss = criterion(output, data)\n",
    "                # ===================backward====================\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss\n",
    "                i += 1\n",
    "            # ===================log========================\n",
    "            if avg_loss.data[0] > prev_avg_loss:\n",
    "                patience += 1\n",
    "            print('epoch [{}/{}], loss:{}, patience: {}'\n",
    "                  .format(epoch + 1, self.num_epochs, avg_loss.data[0] / i, patience))\n",
    "            if patience >= self.early_stop:\n",
    "                break\n",
    "            prev_avg_loss = avg_loss.data[0]\n",
    "        self.model = model\n",
    "        return model\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.model = self.fit(X)\n",
    "        return self.transorm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all avaiulable Manifold Methods\n",
    "import multidimensional\n",
    "import multidimensional.common\n",
    "import multidimensional.mds \n",
    "import multidimensional.smacof\n",
    "from sklearn import manifold, decomposition\n",
    "\n",
    "class IdentityData(object):\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def fit_transform(self, x):\n",
    "        return x\n",
    "\n",
    "def get_manifold_methods(original_dim, target_dim):\n",
    "    method_n_comp = 66\n",
    "    radius_barrier = 1e-3\n",
    "    explore_dim_percent = 1\n",
    "    starting_radius =32\n",
    "    max_turns = 100\n",
    "    point_filter = (multidimensional.point_filters.FixedStochasticFilter(keep_percent=1, recalculate_each=10))\n",
    "    radius_update = (multidimensional.radius_updates.AdaRadiusHalving(tolerance=.5*1e-3, burnout_tolerance=100000))\n",
    "\n",
    "    mds_obj = multidimensional.mds.MDS(target_dim, point_filter, radius_update, starting_radius=starting_radius, \n",
    "                                       radius_barrier=radius_barrier,\n",
    "                max_turns=max_turns, keep_history=False,\n",
    "                explore_dim_percent=explore_dim_percent)\n",
    "\n",
    "    manifold_methods = {\n",
    "        'Pattern Search MDS': { 'results': {}, 'object': multidimensional.mds.MDS(target_dim, point_filter, \n",
    "                                                         radius_update, starting_radius=starting_radius, \n",
    "                                                         radius_barrier=radius_barrier, max_turns=max_turns, \n",
    "                                                         keep_history=False,\n",
    "                                                         dissimilarities='precomputed',\n",
    "                                                         explore_dim_percent=explore_dim_percent)},\n",
    "        'MDS SMACOF': { 'results': {}, 'object': multidimensional.smacof.MDS(n_components=target_dim, n_init=1, \n",
    "                                                 max_iter=max_turns, dissimilarity='euclidean', n_jobs=8)},\n",
    "        'LTSA': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='ltsa',n_jobs=8)},\n",
    "        'Modified LLE': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='modified',n_jobs=8)},\n",
    "        'Hessian LLE': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='hessian',n_jobs=8)},\n",
    "        'LLE': { 'results': {}, 'object': manifold.LocallyLinearEmbedding(method_n_comp, target_dim, \n",
    "                                           eigen_solver='auto', method='standard',n_jobs=8)},\n",
    "        'Truncated SVD': { 'results': {}, 'object': decomposition.TruncatedSVD(n_components=target_dim)},\n",
    "        'Spectral Embedding': { 'results': {}, 'object': manifold.SpectralEmbedding(n_components=target_dim, \n",
    "                                                                                    n_jobs=8)},\n",
    "        'TSNE': { 'results': {}, 'object': manifold.TSNE(n_components=target_dim)},\n",
    "        'ISOMAP': { 'results': {}, 'object': manifold.Isomap(12, target_dim)},\n",
    "        'Original Data': { 'results': {}, 'object': IdentityData()},\n",
    "        'Autoencoder': { 'results': {}, 'object': AE(original_dim, target_dim) }\n",
    "\n",
    "    }\n",
    "    return manifold_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_DR(target_dims, methods_to_test, data_dic, included_sessions, saveto=None):\n",
    "    X_all, Y_all, speaker_indices, number_of_sessions = get_dataset_in_one_array(data_dic,\n",
    "                                                                                included_sessions=included_sessions)\n",
    "    # normalize the input vectors \n",
    "    X_high = StandardScaler().fit_transform(X_all)\n",
    "    \n",
    "    print(X_high.shape)\n",
    "    \n",
    "    reduced = {}\n",
    "    original_dim = X_all.shape[1]\n",
    "    for target_dim in target_dims:\n",
    "        reduced[target_dim] = {}\n",
    "        manifold_methods = get_manifold_methods(original_dim, target_dim)\n",
    "    #     methods_to_test = manifold_methods.keys()\n",
    "        methods_metrics = {}\n",
    "        for selected_method in methods_to_test:            \n",
    "            print('Checking Method: {}'.format(selected_method))\n",
    "            \n",
    "            print('Reducing Input from Dimension: {} to a Lower Embedded Manifold with dimensions: {}...'.format(\n",
    "                   X_high.shape[1], target_dim))\n",
    "#             try:\n",
    "            obj = manifold_methods[selected_method]['object']\n",
    "            if selected_method == 'Pattern Search MDS':\n",
    "#                     d_goal = multidimensional.common.DISTANCE_MATRIX(X_high.astype(np.float64))\n",
    "                d_goal = 1.0 - np.corrcoef(X_high.astype(np.float64))\n",
    "                np.fill_diagonal(d_goal, 0)\n",
    "                X_low = obj.fit_transform(d_goal)\n",
    "            else:\n",
    "                X_low = obj.fit_transform(X_high)\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 X_low = None\n",
    "            reduced[target_dim][selected_method] = X_low\n",
    "    if saveto is not None:\n",
    "        with open(os.path.join('../', 'cache', saveto), 'wb') as fd:\n",
    "            pickle.dump(reduced, fd)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the best performing nonlinear features for KNN classification after dimensionality reduction\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pprint \n",
    "import pandas as pd \n",
    "\n",
    "def run_IEMOCAP_session_KNN(n_neighbors, reduced, data_dic, included_sessions):\n",
    "    _, Y_all, speaker_indices, number_of_sessions = get_dataset_in_one_array(\n",
    "        data_dic, included_sessions=included_sessions)\n",
    "    df_results = {}\n",
    "    for target_dim, methods in reduced.iteritems():\n",
    "        methods_metrics = {}\n",
    "        for selected_method, X_low in methods.iteritems():\n",
    "            print('Checking Method: {}'.format(selected_method))\n",
    "            metrics_l = {'uw_acc': dict([(k, 0.0) for k in n_neighbors]), 'w_acc': dict([(k, 0.0) for k in n_neighbors])}\n",
    "            if X_low is None:\n",
    "                methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "                methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "                continue\n",
    "                \n",
    "            for k in n_neighbors:\n",
    "    #             print 'Testing for Nearest Neighbors: K={}'.format(k)\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', leaf_size=30, \n",
    "                                           p=2, metric='minkowski', metric_params=None, n_jobs=8)\n",
    "\n",
    "                session_folds = generate_session_folds(X_low, Y_all, data_dic, speaker_indices)\n",
    "                for session, X_te, Y_te, X_tr, Y_tr in session_folds:\n",
    "    #                 print \"Testing for Session: {}\".format(session)\n",
    "                    Y_te, Y_tr = fuse_excited_happiness(Y_te), fuse_excited_happiness(Y_tr)\n",
    "                    \n",
    "                    try:\n",
    "                        knn.fit(X_tr, Y_tr) \n",
    "                        Y_predicted = knn.predict(X_te)\n",
    "\n",
    "                        w_acc = accuracy_score(Y_predicted, Y_te)\n",
    "                        cmat = confusion_matrix(Y_te, Y_predicted)\n",
    "                        with np.errstate(divide='ignore'):\n",
    "                            uw_acc = (cmat.diagonal() / (1.0 * cmat.sum(axis=1) + 1e-6)).mean()\n",
    "                            if np.isnan(uw_acc):\n",
    "                                uw_acc = 0.\n",
    "                        w_acc = round(w_acc*100,1)\n",
    "                        uw_acc = round(uw_acc*100,1)\n",
    "                        metrics_l['uw_acc'][k] += uw_acc/number_of_sessions\n",
    "                        metrics_l['w_acc'][k] += w_acc/number_of_sessions\n",
    "                    except:\n",
    "                        metrics_l['uw_acc'][k] += 0.\n",
    "                        metrics_l['w_acc'][k] += 0.\n",
    "    #             print 'Done'\n",
    "            methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "            methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "#             pprint.pprint(metrics_l)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(methods_metrics, orient=\"index\")\n",
    "        df_results[target_dim] = df[sorted(df.columns)]\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for IEMOCAP Session Experiments \n",
    "n_neighbors = np.arange(1, 40, 4)\n",
    "target_dims = [4]\n",
    "\n",
    "# Find all appropriate files \n",
    "IEMOCAP_data_path = '/home/geopar/projects/nldr_visual_recognition/all_TRUE_IEMOCAP_feats/'\n",
    "l_feats_p = IEMOCAP_data_path + 'linear/IEMOCAP_linear_emobase2010'\n",
    "# nl_feats_l = glob.glob( IEMOCAP_data_path + '/utterance/*.dat')\n",
    "# nl_feats_p = nl_feats_l.pop()\n",
    "nl_feats_p = os.path.join(IEMOCAP_data_path, \n",
    "             'utterance/IEMOCAP-rqa-ad_hoc-tau-7-supremum-recurrence_rate-0.15-dur-0.03-fs-16000.dat')\n",
    "included_sessions=['Ses01', 'Ses02', 'Ses03', 'Ses04', 'Ses05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531, 2014)\n",
      "Checking Method: Autoencoder\n",
      "Reducing Input from Dimension: 2014 to a Lower Embedded Manifold with dimensions: 4...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected torch.FloatTensor (got torch.DoubleTensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-1af72731c77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# methods_to_test = ['Truncated SVD', 'Spectral Embedding']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIEMOCAP_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fused_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_feats_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnl_feats_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_DR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods_to_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluded_sessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# original_results = run_IEMOCAP_session_KNN(n_neighbors, [2014], ['Original Data'], data_dic, included_sessions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfused_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_IEMOCAP_session_KNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluded_sessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-a139473560b1>\u001b[0m in \u001b[0;36mrun_DR\u001b[0;34m(target_dims, methods_to_test, data_dic, included_sessions, saveto)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mX_low\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_goal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mX_low\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_high\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#             except Exception as e:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#                 print(e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-640d04684e82>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-640d04684e82>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdvc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# ===================forward=====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected torch.FloatTensor (got torch.DoubleTensor)"
     ]
    }
   ],
   "source": [
    "included_sessions=['Ses01', 'Ses02', 'Ses03', 'Ses04', 'Ses05']\n",
    "methods_to_test = ['Pattern Search MDS', 'Truncated SVD', 'Spectral Embedding', \n",
    "                   'LLE', 'Hessian LLE', 'Modified LLE', 'LTSA', 'ISOMAP']   \n",
    "methods_to_test = ['LLE', 'Truncated SVD', 'Hessian LLE', 'Modified LLE', 'Spectral Embedding', 'ISOMAP', 'LTSA']   \n",
    "methods_to_test = ['Autoencoder']   \n",
    "\n",
    "# methods_to_test = ['Truncated SVD', 'Spectral Embedding']\n",
    "data_dic = IEMOCAP_loader.get_fused_features([l_feats_p, nl_feats_p])\n",
    "reduced = run_DR(target_dims, methods_to_test, data_dic, included_sessions, saveto='test.p')\n",
    "# original_results = run_IEMOCAP_session_KNN(n_neighbors, [2014], ['Original Data'], data_dic, included_sessions)\n",
    "fused_results = run_IEMOCAP_session_KNN(n_neighbors, reduced, data_dic, included_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Target Dimension: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>9</th>\n",
       "      <th>13</th>\n",
       "      <th>17</th>\n",
       "      <th>21</th>\n",
       "      <th>25</th>\n",
       "      <th>29</th>\n",
       "      <th>33</th>\n",
       "      <th>37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hessian LLE UA</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hessian LLE WA</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISOMAP UA</th>\n",
       "      <td>39.24</td>\n",
       "      <td>41.68</td>\n",
       "      <td>44.74</td>\n",
       "      <td>46.14</td>\n",
       "      <td>47.54</td>\n",
       "      <td>47.30</td>\n",
       "      <td>47.14</td>\n",
       "      <td>47.28</td>\n",
       "      <td>48.18</td>\n",
       "      <td>48.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISOMAP WA</th>\n",
       "      <td>38.78</td>\n",
       "      <td>40.90</td>\n",
       "      <td>44.06</td>\n",
       "      <td>45.18</td>\n",
       "      <td>46.48</td>\n",
       "      <td>46.26</td>\n",
       "      <td>46.16</td>\n",
       "      <td>46.30</td>\n",
       "      <td>47.20</td>\n",
       "      <td>47.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLE UA</th>\n",
       "      <td>40.80</td>\n",
       "      <td>43.64</td>\n",
       "      <td>44.82</td>\n",
       "      <td>45.58</td>\n",
       "      <td>45.60</td>\n",
       "      <td>46.02</td>\n",
       "      <td>46.32</td>\n",
       "      <td>46.46</td>\n",
       "      <td>46.22</td>\n",
       "      <td>46.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLE WA</th>\n",
       "      <td>40.26</td>\n",
       "      <td>43.16</td>\n",
       "      <td>44.36</td>\n",
       "      <td>45.02</td>\n",
       "      <td>45.00</td>\n",
       "      <td>45.44</td>\n",
       "      <td>45.64</td>\n",
       "      <td>45.84</td>\n",
       "      <td>45.48</td>\n",
       "      <td>45.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTSA UA</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTSA WA</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modified LLE UA</th>\n",
       "      <td>40.68</td>\n",
       "      <td>44.88</td>\n",
       "      <td>46.72</td>\n",
       "      <td>47.66</td>\n",
       "      <td>47.72</td>\n",
       "      <td>47.90</td>\n",
       "      <td>48.48</td>\n",
       "      <td>47.98</td>\n",
       "      <td>48.22</td>\n",
       "      <td>48.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modified LLE WA</th>\n",
       "      <td>40.04</td>\n",
       "      <td>43.52</td>\n",
       "      <td>45.48</td>\n",
       "      <td>46.78</td>\n",
       "      <td>47.04</td>\n",
       "      <td>47.38</td>\n",
       "      <td>47.96</td>\n",
       "      <td>47.42</td>\n",
       "      <td>47.64</td>\n",
       "      <td>47.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spectral Embedding UA</th>\n",
       "      <td>42.18</td>\n",
       "      <td>44.80</td>\n",
       "      <td>47.32</td>\n",
       "      <td>48.10</td>\n",
       "      <td>48.82</td>\n",
       "      <td>48.68</td>\n",
       "      <td>49.14</td>\n",
       "      <td>49.48</td>\n",
       "      <td>49.52</td>\n",
       "      <td>49.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spectral Embedding WA</th>\n",
       "      <td>41.70</td>\n",
       "      <td>43.86</td>\n",
       "      <td>46.36</td>\n",
       "      <td>47.04</td>\n",
       "      <td>47.76</td>\n",
       "      <td>47.56</td>\n",
       "      <td>48.14</td>\n",
       "      <td>48.54</td>\n",
       "      <td>48.52</td>\n",
       "      <td>48.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Truncated SVD UA</th>\n",
       "      <td>41.84</td>\n",
       "      <td>45.34</td>\n",
       "      <td>48.30</td>\n",
       "      <td>49.56</td>\n",
       "      <td>50.16</td>\n",
       "      <td>50.06</td>\n",
       "      <td>51.40</td>\n",
       "      <td>51.50</td>\n",
       "      <td>51.74</td>\n",
       "      <td>51.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Truncated SVD WA</th>\n",
       "      <td>41.38</td>\n",
       "      <td>44.56</td>\n",
       "      <td>47.26</td>\n",
       "      <td>48.78</td>\n",
       "      <td>49.32</td>\n",
       "      <td>49.34</td>\n",
       "      <td>50.56</td>\n",
       "      <td>50.60</td>\n",
       "      <td>50.82</td>\n",
       "      <td>50.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          1      5      9      13     17     21     25     29  \\\n",
       "Hessian LLE UA          0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "Hessian LLE WA          0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "ISOMAP UA              39.24  41.68  44.74  46.14  47.54  47.30  47.14  47.28   \n",
       "ISOMAP WA              38.78  40.90  44.06  45.18  46.48  46.26  46.16  46.30   \n",
       "LLE UA                 40.80  43.64  44.82  45.58  45.60  46.02  46.32  46.46   \n",
       "LLE WA                 40.26  43.16  44.36  45.02  45.00  45.44  45.64  45.84   \n",
       "LTSA UA                 0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "LTSA WA                 0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "Modified LLE UA        40.68  44.88  46.72  47.66  47.72  47.90  48.48  47.98   \n",
       "Modified LLE WA        40.04  43.52  45.48  46.78  47.04  47.38  47.96  47.42   \n",
       "Spectral Embedding UA  42.18  44.80  47.32  48.10  48.82  48.68  49.14  49.48   \n",
       "Spectral Embedding WA  41.70  43.86  46.36  47.04  47.76  47.56  48.14  48.54   \n",
       "Truncated SVD UA       41.84  45.34  48.30  49.56  50.16  50.06  51.40  51.50   \n",
       "Truncated SVD WA       41.38  44.56  47.26  48.78  49.32  49.34  50.56  50.60   \n",
       "\n",
       "                          33     37  \n",
       "Hessian LLE UA          0.00   0.00  \n",
       "Hessian LLE WA          0.00   0.00  \n",
       "ISOMAP UA              48.18  48.10  \n",
       "ISOMAP WA              47.20  47.04  \n",
       "LLE UA                 46.22  46.14  \n",
       "LLE WA                 45.48  45.50  \n",
       "LTSA UA                 0.00   0.00  \n",
       "LTSA WA                 0.00   0.00  \n",
       "Modified LLE UA        48.22  48.20  \n",
       "Modified LLE WA        47.64  47.52  \n",
       "Spectral Embedding UA  49.52  49.06  \n",
       "Spectral Embedding WA  48.52  48.08  \n",
       "Truncated SVD UA       51.74  51.42  \n",
       "Truncated SVD WA       50.82  50.46  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "# for target_dim in sorted(original_results.keys()):\n",
    "#     df = original_results[target_dim]\n",
    "#     print \"Using Original Data\"\n",
    "#     print display(df)\n",
    "\n",
    "for target_dim in sorted(fused_results.keys()):\n",
    "    df = fused_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dic = IEMOCAP_loader.get_fused_features([nl_feats_p])\n",
    "original_results = run_IEMOCAP_session_KNN(n_neighbors, [432], ['Original Data'], data_dic, included_sessions)\n",
    "rqa_results = run_IEMOCAP_session_KNN(n_neighbors, target_dims, methods_to_test, data_dic, included_sessions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for target_dim in sorted(original_results.keys()):\n",
    "    df = original_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "\n",
    "for target_dim in sorted(rqa_results.keys()):\n",
    "    df = rqa_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic = IEMOCAP_loader.get_fused_features([l_feats_p])\n",
    "original_results = run_IEMOCAP_session_KNN(n_neighbors, [1582], ['Original Data'], data_dic, included_sessions)\n",
    "emobase_results = run_IEMOCAP_session_KNN(n_neighbors, target_dims, methods_to_test, data_dic, included_sessions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_dim in sorted(original_results.keys()):\n",
    "    df = original_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "\n",
    "for target_dim in sorted(emobase_results.keys()):\n",
    "    df = emobase_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the experiment for EmoDB Speaker independent Experiments\n",
    "def get_dataset_for_all_speakers(features_dic):\n",
    "    speaker_indices = {}\n",
    "    x_all_list = []\n",
    "    Y_all = []\n",
    "    prev_ind = 0\n",
    "    for te_speaker, te_data in features_dic.items():  \n",
    "        x_all_list.append(te_data['x'])\n",
    "        Y_all += te_data['y']\n",
    "        this_speaker_samples = len(te_data['y'])\n",
    "        \n",
    "        speaker_indices[te_speaker] = (prev_ind, prev_ind + this_speaker_samples)\n",
    "        prev_ind += this_speaker_samples\n",
    "        X_all = np.concatenate(x_all_list, axis=0)\n",
    "    number_of_speakers = len(features_dic.keys())\n",
    "    return X_all, Y_all, speaker_indices, number_of_speakers\n",
    "\n",
    "def generate_speaker_independent_folds(X_all, Y_all, features_dic, speaker_indices):\n",
    "    sorted_speakers = sorted(speaker_indices.keys())\n",
    "    for (te_speaker, (st, et)) in speaker_indices.items():\n",
    "        Y_te = Y_all[st:et]\n",
    "        X_te = X_all[st:et, :]\n",
    "        \n",
    "        x_tr_list = []\n",
    "        Y_tr = []\n",
    "        for sp in sorted_speakers:\n",
    "            if sp == te_speaker:\n",
    "                continue\n",
    "            st, et = speaker_indices[sp] \n",
    "            x_tr_list.append(X_all[st:et, :])\n",
    "            Y_tr += Y_all[st:et]\n",
    "        X_tr = np.concatenate(x_tr_list, axis=0)    \n",
    "        \n",
    "        yield te_speaker, X_te, Y_te, X_tr, Y_tr \n",
    "\n",
    "\n",
    "def run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic):\n",
    "\n",
    "    X_all, Y_all, speaker_indices, number_of_speakers = get_dataset_for_all_speakers(data_dic)\n",
    "    df_results = {}\n",
    "    # normalize the input vectors \n",
    "    X_high = StandardScaler().fit_transform(X_all)\n",
    "    \n",
    "    print X_high.shape \n",
    "\n",
    "    for target_dim in target_dims:\n",
    "        print \"Running for Target Dimensions={}\".format(target_dim)\n",
    "        manifold_methods = get_manifold_methods(target_dim)\n",
    "        methods_metrics = {}\n",
    "        for selected_method in methods_to_test:\n",
    "            metrics_l = {'uw_acc': dict([(k, 0.0) for k in n_neighbors]), \n",
    "                         'w_acc': dict([(k, 0.0) for k in n_neighbors])}\n",
    "            print 'Checking Method: {}'.format(selected_method)\n",
    "            try:\n",
    "                print 'Reducing Input from Dimension: {} to a Lower Embedded Manifold with dimensions: {}...'.format(\n",
    "                   X_high.shape[1], target_dim)\n",
    "                obj = manifold_methods[selected_method]['object']\n",
    "                if selected_method == 'Pattern Search MDS':\n",
    "                    d_goal = multidimensional.common.DISTANCE_MATRIX(X_high.astype(np.float64))\n",
    "                    X_low = obj.fit_transform(d_goal)\n",
    "                else:\n",
    "                    X_low = obj.fit_transform(X_high)\n",
    "\n",
    "            except:\n",
    "                methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "                methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "                continue                    \n",
    "            \n",
    "            for k in n_neighbors:\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', leaf_size=30, \n",
    "                                           p=2, metric='minkowski', metric_params=None, n_jobs=8)\n",
    "\n",
    "                speaker_folds = generate_speaker_independent_folds(X_low, Y_all, data_dic, speaker_indices)\n",
    "                for te_speaker, X_te, Y_te, X_tr, Y_tr in speaker_folds:                    \n",
    "                    try:\n",
    "                        knn.fit(X_tr, Y_tr) \n",
    "                        Y_predicted = knn.predict(X_te)\n",
    "\n",
    "                        w_acc = accuracy_score(Y_predicted, Y_te)\n",
    "                        cmat = confusion_matrix(Y_te, Y_predicted)\n",
    "                        with np.errstate(divide='ignore'):\n",
    "                            uw_acc = (cmat.diagonal() / (1.0 * cmat.sum(axis=1) + 1e-6)).mean()\n",
    "                            if np.isnan(uw_acc):\n",
    "                                uw_acc = 0.\n",
    "                        w_acc = round(w_acc*100,0)\n",
    "                        uw_acc = round(uw_acc*100,)\n",
    "                        metrics_l['uw_acc'][k] += uw_acc/number_of_speakers\n",
    "                        metrics_l['w_acc'][k] += w_acc/number_of_speakers\n",
    "                    except:\n",
    "                        metrics_l['uw_acc'][k] += 0.\n",
    "                        metrics_l['w_acc'][k] += 0.\n",
    "                    \n",
    "            methods_metrics[selected_method+' UA'] = metrics_l['uw_acc']\n",
    "            methods_metrics[selected_method+' WA'] = metrics_l['w_acc']\n",
    "\n",
    "        df = pd.DataFrame.from_dict(methods_metrics, orient=\"index\")\n",
    "        df_results[target_dim] = df[sorted(df.columns)]\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for Speaker Independent Experiments \n",
    "n_neighbors = np.arange(1, 40, 4)\n",
    "target_dims = [2, 5, 10, 25]\n",
    "\n",
    "# Find all appropriate files \n",
    "data_path = '/home/thymios/all_BERLIN_features/'\n",
    "berlin_l_feats_p = data_path + 'linear/BERLIN_linear_emobase2010'\n",
    "# nl_feats_l = glob.glob( IEMOCAP_data_path + '/utterance/*.dat')\n",
    "# nl_feats_p = nl_feats_l.pop()\n",
    "berlin_nl_feats_p = os.path.join(data_path, \n",
    "             'rqa/utterance/BERLIN-rqa-ad_hoc-tau-7-manhattan-recurrence_rate-0.15-dur-0.02-fs-16000.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_to_test = ['Pattern Search MDS', 'MDS SMACOF','Truncated SVD', 'Spectral Embedding', 'LLE', \n",
    "                   'Hessian LLE', 'Modified LLE', 'LTSA', 'ISOMAP']   \n",
    "# methods_to_test = ['Pattern Search MDS']\n",
    "# methods_to_test = ['Truncated SVD']\n",
    "\n",
    "data_dic = IEMOCAP_loader.get_fused_features([berlin_nl_feats_p])\n",
    "berlin_original_nl_results = run_speaker_independent_KNN(n_neighbors, [2014], ['Original Data'], data_dic)\n",
    "berlin_nl_results = run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic)\n",
    "\n",
    "data_dic = IEMOCAP_loader.get_fused_features([berlin_l_feats_p, berlin_nl_feats_p])\n",
    "berlin_original_fused_results = run_speaker_independent_KNN(n_neighbors, [2014], ['Original Data'], data_dic)\n",
    "berlin_fused_results = run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic)\n",
    "\n",
    "data_dic = IEMOCAP_loader.get_fused_features([berlin_l_feats_p])\n",
    "berlin_original_l_results = run_speaker_independent_KNN(n_neighbors, [2014], ['Original Data'], data_dic)\n",
    "berlin_l_results = run_speaker_independent_KNN(n_neighbors, target_dims, methods_to_test, data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "print \"Using RQA Feature Set and Dimensionality Reduction...\"\n",
    "\n",
    "def latex_preformat_print(df):\n",
    "    methods = {}\n",
    "    for ind in df.index.values:\n",
    "        if not ind[:-3] in methods and ind[-2:] == 'WA':\n",
    "            methods[ind[:-3]] = list(df[[1,5,9,13,17,21]].loc[ind])\n",
    "    for ind in df.index.values:\n",
    "        if ind[-2:] == 'UA':\n",
    "            methods[ind[:-3]] += list(df[[1,5,9,13,17,21]].loc[ind])\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(methods, orient=\"index\")\n",
    "    print df.to_latex()\n",
    "\n",
    "for target_dim in sorted(berlin_original_nl_results.keys()):\n",
    "    df = berlin_original_nl_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)\n",
    "    \n",
    "for target_dim in sorted(berlin_nl_results.keys()):\n",
    "    df = berlin_nl_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "print \"Using Emobase Features and Dimensionality Reduction...\"\n",
    "\n",
    "for target_dim in sorted(berlin_original_l_results.keys()):\n",
    "    df = berlin_original_l_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)\n",
    "\n",
    "for target_dim in sorted(berlin_l_results.keys()):\n",
    "    df = berlin_l_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "print \"Using Fused Features and Dimensionality Reduction...\"\n",
    "\n",
    "for target_dim in sorted(berlin_original_fused_results.keys()):\n",
    "    df = berlin_original_fused_results[target_dim]\n",
    "    print \"Using Original Data\"\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)\n",
    "\n",
    "for target_dim in sorted(berlin_fused_results.keys()):\n",
    "    df = berlin_fused_results[target_dim]\n",
    "    print \"For Target Dimension: {}\".format(target_dim)\n",
    "    print display(df)\n",
    "    latex_preformat_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
